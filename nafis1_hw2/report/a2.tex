\documentclass{article}
\usepackage{fullpage}
\usepackage{color}
\usepackage{amsmath}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{amssymb}
\usepackage{listings} % For displaying code
\begin{document}

% Colors
\definecolor{blu}{rgb}{0,0,1}
\def\blu#1{{\color{blu}#1}}
\definecolor{gre}{rgb}{0,.5,0}
\def\gre#1{{\color{gre}#1}}
\definecolor{red}{rgb}{1,0,0}
\def\red#1{{\color{red}#1}}
\def\norm#1{\|#1\|}
\newcommand{\argmin}[1]{\mathop{\hbox{argmin}}_{#1}}
\newcommand{\argmax}[1]{\mathop{\hbox{argmax}}_{#1}}
\def\R{\mathbb{R}}
\newcommand{\fig}[2]{\includegraphics[width=#1\textwidth]{#2}}
\newcommand{\centerfig}[2]{\begin{center}\includegraphics[width=#1\textwidth]{#2}\end{center}}
\def\items#1{\begin{itemize}#1\end{itemize}}
\def\enum#1{\begin{enumerate}#1\end{enumerate}}
\def\answer#1{\iftoggle{answers}{\blu{Answer}:\\#1}}
\def\rubric#1{Rubric: \{#1\}}{}
\newcommand{\code}[1]{\lstinputlisting[language=Matlab]{#1}}


\title{CPSC 340 Assignment 2}
\author{K-Nearest Neighbours, Random Forests, K-Means, Density-Based Clustering}
\date{}
\maketitle

\section{K-Nearest Neighbours}
\subsection{KNN Prediction}
\begin{enumerate}
\item See predict function in knn.py. \url{https://github.ubc.ca/cpsc340/jeanlam_nafis1_hw2/blob/master/code/knn.py}
\item The training and test errors:\\
For K=1:Training error:0 Testing error:0.0645\\
For K=3:Training error:0.0275 Testing error:0.066\\
For K=10:Training error:0.072 Testing error:0.097\\
\item \centerfig{.5}{../figs/{1.1}.png}
\item For k=1 the training error is 0 because since k =1 it is 1-nearest neighbour. When
we our function is predicting we are just taking the labels from the training data.The training
examples are 1-nearest neighbour of itself.
\item We can use cross-validation like 5-fold or 10-fold to choose k.
\end{enumerate}
\subsection{Condensed Nearest Neighbours}
\begin{enumerate}
\item See cnn.py. \url{https://github.ubc.ca/cpsc340/jeanlam_nafis1_hw2/blob/master/code/cnn.py}
\item Training error: 0.0075, Testing error:0.0175, Number of variables:455
\item See figures. \centerfig{.5}{../figs/{1.2}.png}
\item The examples that were correctly classified are no longer being stored as we took a subset of the training examples. As we add more examples the previous examples can become incorrectly classified
\item Now we are computing only the s distances to t test points.This causes us to have a runtime of O(dst).O(s) for s distances,O(t) for t points and O(d).
\item One reason for the test error being that high is some states might not be present in the training data.The training error is high because the order of the example affects the CNN method

\end{enumerate}

\section{Random Forests}
\subsection{Random Trees}
\begin{enumerate}
\item \centerfig{.5}{../figs/q2_1_1.pdf}
\item The decision tree function terminates when the max depth is set to infinity as there are no more variables to split on. The fit function returns when either the max-depth is 1, or there is no split variable in the model.
\item \centerfig{.5}{../figs/q2_1_3.pdf} The random stump model can be found at \url{https://github.ubc.ca/cpsc340/jeanlam_nafis1_hw2/blob/master/code/random_stump.py}
\item \centerfig{.5}{../figs/q2_1_4.pdf}
\end{enumerate}
\subsection{Random Decision Forests}
\begin{enumerate}
\item The test error for 50 trees and infinite depth is 0.367 and the training error is 0.0
\item The test error for 50 trees and infinite depth with bootstrapping is 0.610 and the training error is 0.473
\item The test error for 50 random trees with infinite depth is 0.178 and the training error is 0.0
\item The test error for 50 random trees with infinite depth and bootstrapping is 0.568 and the training error is 0.458 \\
The random forest model can be found at \url{https://github.ubc.ca/cpsc340/jeanlam_nafis1_hw2/blob/master/code/random_forest.py}
\item From the above test errors, it is evident that using a random tree without bootstrapping works better than a decision tree without bootstrapping. 
\end{enumerate}

\section{K-Means Clustering}
\subsection{Selecting among Initializations}
\begin{enumerate}
\item The error function can be found at \url{https://github.ubc.ca/cpsc340/jeanlam_nafis1_hw2/blob/master/code/kmeans.py}
\item \centerfig{.5}{../figs/q3_1_2.pdf} The minimum error obtained is 1062.014
\end{enumerate}
\subsection{Selecting k}
\begin{enumerate}
\item We cannot choose k by minimizing the sum of squares distance as we have many possible ${w_c}$ closest means. We must consider N-1 different k values, and for each k value, determine the k means. For large datasets, this is not feasible as it is computationally heavy. Also the function would choose the largest value of k as the function decreases with k.
\item As mentioned above, evaluating the objective function on test data results in the same problem. As well, we would not have a good model to predict on if we are only using test data.As k increases so does the number of clusters.
\item \centerfig{.5}{../figs/q3_2_3.pdf} Dashes were used to connect the plot to more clearly view the minimum points at each k value for the next question. 
\item The largest change in slope is found between k=1 and k=2. According to this method, k=2 is a suitable k value for this dataset as it provides the most "benefit". The slope between k=1 and k=2 is -3757.885, signifying a large improvement in test error with the addition of one more cluster. As well, the slope between k=2 and k=3 is large, at 2181.363. Beginning at k=4, changes to the error are not drastic, and vary by less than 50. This means that adding one more cluster does not change the overall error by very much. Overall, choosing k=4 is most appropriate as the change in error for subsequent cluster numbers is minimal. 
\end{enumerate}
\subsection{k-Medians}
\begin{enumerate}
\item \centerfig{.5}{../figs/q3_3_1.pdf} The lowest error obtained over 50 runs is 1667.278.
\item Approaching the same methodology as in 3.2.4, I ran k-means 50 times, varying k between [1,10]. The following is the plot obtained showing the minimum error obtained in each k. \centerfig{.5}{../figs/q3_3_2.pdf} It is apparent, just like in 3.2.4, that the largest slope occured between k=1 and k=2, with value -3773.338. The same is true between k=2 and k=3, and k=3 and k=4. Beginning at k=4, however, the difference in error is minimal (${<}$10) when adding one more cluster. Thus, a k-value of 4 may be most appropriate for this dataset.
\item The k-medians algorithm can be found at \url{https://github.ubc.ca/cpsc340/jeanlam_nafis1_hw2/blob/master/code/kmedian.py}. The error obtained is 2082.490.
\item Under the elbow method with running k-median 50 times each over varying k-values of [1-10], the following graph was obtained: \centerfig{.5}{../figs/q3_3_4.pdf} Following the trend, the largest change in slope appears between k=1 and k=2, with a change of -4948.097. K=4 is the most appropriate choice as when k=5 was run, there was a larger error than k=4. 
\end{enumerate}

\section{Vector Quantization and Density-Based Clustering}
\subsection{Image Colour-Space Compression}
\begin{enumerate}
\item Link to code: \url{https://github.ubc.ca/cpsc340/jeanlam_nafis1_hw2/blob/master/code/quantize_image.py}
\item Please see doge images here: \url{https://github.ubc.ca/cpsc340/jeanlam_nafis1_hw2/tree/master/figs} or embedded below\\
One bit:
\centerfig{.5}{../figs/{4.1k1}.png} Two bits:
\centerfig{.5}{../figs/{4.1k2}.png} Four bits:
\centerfig{.5}{../figs/{4.1k4}.png} Six bits:
\centerfig{.5}{../figs/{4.1k6}.png} 

\end{enumerate}
\subsection{Effect of Parameters on DBSCAN}

 Keeping minPts = 3 \\
 The 4 true cluster, radius =11\\
 3 clusters,radius =18\\
 2 clusters, radius =180\\
 1 clusters, radius =800\\

\subsection{K-Means vs. DBSCAN Clustering}
\begin{enumerate} 
\item I have chosen r = 15 and kept min =3pts to get the following clusters:\\
{Cluster 1: antelope horse moose ox sheep giraffe buffalo zebra deer pig cow \\
Cluster 2: dalmatian persian+cat german+shepherd siamese+cat mole tiger leopard fox hamster squirrel rabbit wolf chihuahua rat weasel bobcat mouse collie\\
Cluster 3: hippopotamus elephant rhinoceros\\
Cluster 4: blue+whale humpback+whale seal walrus dolphin\\
Cluster 5: spider+monkey gorilla chimpanzee}
\end{enumerate}
\end{document}