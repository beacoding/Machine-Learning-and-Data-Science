\documentclass{article}
\usepackage{fullpage}
\usepackage{color}
\usepackage{amsmath}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{amssymb}
\usepackage{listings} % For displaying code
\begin{document}

% Colors
\definecolor{blu}{rgb}{0,0,1}
\def\blu#1{{\color{blu}#1}}
\definecolor{gre}{rgb}{0,.5,0}
\def\gre#1{{\color{gre}#1}}
\definecolor{red}{rgb}{1,0,0}
\def\red#1{{\color{red}#1}}
\def\norm#1{\|#1\|}
\newcommand{\argmin}[1]{\mathop{\hbox{argmin}}_{#1}}
\newcommand{\argmax}[1]{\mathop{\hbox{argmax}}_{#1}}
\def\R{\mathbb{R}}
\newcommand{\fig}[2]{\includegraphics[width=#1\textwidth]{#2}}
\newcommand{\centerfig}[2]{\begin{center}\includegraphics[width=#1\textwidth]{#2}\end{center}}
\def\items#1{\begin{itemize}#1\end{itemize}}
\def\enum#1{\begin{enumerate}#1\end{enumerate}}
\def\answer#1{\iftoggle{answers}{\blu{Answer}:\\#1}}
\def\rubric#1{Rubric: \{#1\}}{}
\newcommand{\code}[1]{\lstinputlisting[language=Matlab]{#1}}


\title{CPSC 340 Assignment 2}
\author{K-Nearest Neighbours, Random Forests, K-Means, Density-Based Clustering}
\date{}
\maketitle

\section{K-Nearest Neighbours}
\subsection{KNN Prediction}
\begin{enumerate}
\item See predict function in knn.py
\item 
\item
\item
\item
\end{enumerate}
\subsection{Condensed Nearest Neighbours}
\begin{enumerate}
\item
\item
\item
\item
\item
\item
\end{enumerate}

\section{Random Forests}
\subsection{Random Trees}
\begin{enumerate}
\item \centerfig{.5}{../figs/q2_1_1.pdf}
\item The decision tree function terminates when the max depth is set to infinity as there are no more variables to split on. The fit function returns when either the max-depth is 1, or there is no split variable in the model.
\item \centerfig{.5}{../figs/q2_1_3.pdf} The random stump model can be found at \url{https://github.ubc.ca/cpsc340/jeanlam_nafis1_hw2/blob/master/code/random_stump.py}
\item \centerfig{.5}{../figs/q2_1_4.pdf}
\end{enumerate}
\subsection{Random Decision Forests}
\begin{enumerate}
\item The test error for 50 trees and infinite depth is 0.367 and the training error is 0.0
\item The test error for 50 trees and infinite depth with bootstrapping is 0.610 and the training error is 0.473
\item The test error for 50 random trees with infinite depth is 0.178 and the training error is 0.0
\item The test error for 50 random trees with infinite depth and bootstrapping is 0.568 and the training error is 0.458 \\
The random forest model can be found at \url{https://github.ubc.ca/cpsc340/jeanlam_nafis1_hw2/blob/master/code/random_forest.py}
\item From the above test errors, it is evident that using a random tree without bootstrapping works better than a decision tree without bootstrapping. 
\end{enumerate}

\section{K-Means Clustering}
\subsection{Selecting among Initializations}
\begin{enumerate}
\item The error function can be found at \url{https://github.ubc.ca/cpsc340/jeanlam_nafis1_hw2/blob/master/code/kmeans.py}
\item \centerfig{.5}{../figs/q3_1_2.pdf} The minimum error obtained is 1062.014
\end{enumerate}
\subsection{Selecting k}
\begin{enumerate}
\item We cannot choose k by minimizing the sum of squares distance as we have many possible ${w_c}$ closest means. We must consider N-1 different k values, and for each k value, determine the k means. For large datasets, this is not feasible as it is computationally heavy. 
\item As mentioned above, evaluating the objective function on test data results in the same problem. As well, we would not have a good model to predict on if we are only using test data. 
\item \centerfig{.5}{../figs/q3_2_3.pdf} Dashes were used to connect the plot to more clearly view the minimum points at each k value for the next question. 
\item The largest change in slope is found between k=1 and k=2. According to this method, k=2 is a suitable k value for this dataset as it provides the most "benefit". The slope between k=1 and k=2 is -3757.885, signifying a large improvement in test error with the addition of one more cluster. As well, the slope between k=2 and k=3 is large, at 2181.363. Beginning at k=4, changes to the error are not drastic, and vary by less than 50. This means that adding one more cluster does not change the overall error by very much. Overall, choosing k=4 is most appropriate as the change in error for subsequent cluster numbers is minimal. 
\end{enumerate}
\subsection{k-Medians}
\begin{enumerate}
\item \centerfig{.5}{../figs/q3_3_1.pdf} The lowest error obtained over 50 runs is 1667.278.
\item Approaching the same methodology as in 3.2.4, I ran k-means 50 times, varying k between [1,10]. The following is the plot obtained showing the minimum error obtained in each k. \centerfig{.5}{../figs/q3_3_2.pdf} It is apparent, just like in 3.2.4, that the largest slope occured between k=1 and k=2, with value -3773.338. The same is true between k=2 and k=3, and k=3 and k=4. Beginning at k=4, however, the difference in error is minimal (${<}$10) when adding one more cluster. Thus, a k-value of 4 may be most appropriate for this dataset.
\item The k-medians algorithm can be found at \url{https://github.ubc.ca/cpsc340/jeanlam_nafis1_hw2/blob/master/code/kmedian.py}. The error obtained is 2082.490.
\item Under the elbow method with running k-median 50 times each over varying k-values of [1-10], the following graph was obtained: \centerfig{.5}{../figs/q3_3_4.pdf} Following the trend, the largest change in slope appears between k=1 and k=2, with a change of -4948.097. K=4 is the most appropriate choice as when k=5 was run, there was a larger error than k=4. 
\end{enumerate}

\section{Vector Quantization and Density-Based Clustering}
\subsection{Image Colour-Space Compression}
\begin{enumerate}
\item
\item
\end{enumerate}
\subsection{Effect of Parameters on DBSCAN}
\begin{enumerate}
\item
\item
\item
\item
\end{enumerate}
\subsection{K-Means vs. DBSCAN Clustering}
\begin{enumerate}
\item
\item
\item
\end{enumerate}
\end{document}