Skip to content
This repository
Search
Pull requests
Issues
Gist
 @nafis1
In order to perform upgrade maintenance, GitHub will be unavailable on Monday, Feb 6, 2017, between 1-2PM.
 Watch 0
  Star 0
 Fork 0 cpsc340/jeanlam_nafis1_hw2 Private
 Code  Issues 0  Pull requests 1  Projects 0  Wiki  Pulse  Graphs
Branch: WIP Find file Copy pathjeanlam_nafis1_hw2/report/report.tex
f27b3f2  8 hours ago
@jeanlam jeanlam 2.2 errors
1 contributor
RawBlameHistory     
115 lines (108 sloc)  5.8 KB
\documentclass{article}
<<<<<<< HEAD



=======
>>>>>>> 8dcb0a0614e9c015521cc23f9a54f9cd0770b303
\usepackage{fullpage}
\usepackage{color}
\usepackage{amsmath}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{amssymb}
\usepackage{listings} % For displaying code
\begin{document}

% Colors
\definecolor{blu}{rgb}{0,0,1}
\def\blu#1{{\color{blu}#1}}
\definecolor{gre}{rgb}{0,.5,0}
\def\gre#1{{\color{gre}#1}}
\definecolor{red}{rgb}{1,0,0}
\def\red#1{{\color{red}#1}}
\def\norm#1{\|#1\|}
\newcommand{\argmin}[1]{\mathop{\hbox{argmin}}_{#1}}
\newcommand{\argmax}[1]{\mathop{\hbox{argmax}}_{#1}}
\def\R{\mathbb{R}}
\newcommand{\fig}[2]{\includegraphics[width=#1\textwidth]{#2}}
\newcommand{\centerfig}[2]{\begin{center}\includegraphics[width=#1\textwidth]{#2}\end{center}}
\def\items#1{\begin{itemize}#1\end{itemize}}
\def\enum#1{\begin{enumerate}#1\end{enumerate}}
\def\answer#1{\iftoggle{answers}{\blu{Answer}:\\#1}}
\def\rubric#1{Rubric: \{#1\}}{}
\newcommand{\code}[1]{\lstinputlisting[language=Matlab]{#1}}


\title{CPSC 340 Assignment 2}
\author{K-Nearest Neighbours, Random Forests, K-Means, Density-Based Clustering}
\date{}
\maketitle

\section{K-Nearest Neighbours}
\subsection{KNN Prediction}
\begin{enumerate}
\item See predict function in knn.py
\item 1.2 The training and test errors:
\item For K=1:Training error:0 Testing error:0.0645
\item For K=3:Training error:0.0275 Testing error:0.066
\item For K=10:Training error:0.072 Testing error:0.097
\item 1.3 Plot can be found in:
\item 1.4 For k=1 the training error is 0 because since k =1 it is 1-nearest neighbour. When
we our function is predicting we are just taking the labels from the training data.The training
examples are 1-nearest neighbour of itself.
\item 1.5 We can use cross-validation like 5-fold or 10-fold to choose k.
\end{enumerate}
\subsection{Condensed Nearest Neighbours}
\begin{enumerate}
\item 2. Training error: 0.0075, Testing error:0.0175, Number of variables:455
\item 3. Plot link:
\item 4. The examples that were correctly classified are no longer being stored as we took a subset of the training examples. As we add more examples the previous examples can become incorrectly classified
\item 5. Now we are computing only the s distances to t test points.This causes us to have a runtime of O(dst).O(s) for s distances,O(t) for t points and O(d).
\item One reason for the test error being that high is some states might not be present in the training data.The training error is high because the order of the example affects the CNN method
\item
\end{enumerate}

\section{Random Forests}
\subsection{Random Trees}
\begin{enumerate}
\item \centerfig{.5}{../figs/q2_1_1.pdf}
\item The decision tree function terminates when the max depth is set to infinity as there are no more variables to split on. The fit function returns when either the max-depth is 1, or there is no split variable in the model.
\item \centerfig{.5}{../figs/q2_1_3.pdf} The random stump model can be found at \url{https://github.ubc.ca/cpsc340/jeanlam_nafis1_hw2/blob/master/code/random_stump.py}
\item \centerfig{.5}{../figs/q2_1_4.pdf}
\end{enumerate}
\subsection{Random Decision Forests}
\begin{enumerate}
\item The test error for 50 trees and infinite depth is 0.367 and the training error is 0.0
\item The test error for 50 trees and infinite depth with bootstrapping is 0.610 and the training error is 0.473
\item The test error for 50 random trees with infinite depth is 0.178 and the training error is 0.0
\item The test error for 50 random trees with infinite depth and bootstrapping is 0.568 and the training error is 0.458 \\
The random forest model can be found at \url{https://github.ubc.ca/cpsc340/jeanlam_nafis1_hw2/blob/master/code/random_forest.py}
\item From the above test errors, it is evident that using a random tree without bootstrapping works better than a decision tree without bootstrapping. 
\end{enumerate}

\section{K-Means Clustering}
\subsection{Selecting among Initializations}
\begin{enumerate}
\item The error function can be found at \url{https://github.ubc.ca/cpsc340/jeanlam_nafis1_hw2/blob/master/code/kmeans.py}
\item \centerfig{.5}{../figs/q3_1_2.pdf} The minimum error obtained is 1062.014
\end{enumerate}
\subsection{Selecting k}
\begin{enumerate}
\item We cannot choose k by minimizing the sum of squares distance as we have many possible ${w_c}$ closest means. We must consider N-1 different k values, and for each k value, determine the k means. For large datasets, this is not feasible as it is computationally heavy. 
\item As mentioned above, evaluating the objective function on test data results in the same problem. As well, we would not have a good model to predict on if we are only using test data.
\item \centerfig{.5}{../figs/q3_2_3.pdf} Dashes were used to connect the plot to more clearly view the minimum points at each k value for the next question. 
\item The largest change in slope is found between k=1 and k=2. According to this method, k=2 is a suitable k value for this dataset as it provides the most "benefit". The slope between k=1 and k=2 is -3757.885, signifying a large improvement in test error with the addition of one more cluster. As well, the slope between k=2 and k=3 is large, at 2181.363. Beginning at k=4, changes to the error are not drastic, and vary by less than 50. This means that adding one more cluster does not change the overall error by very much. Overall, choosing k=4 is most appropriate as the change in error for subsequent cluster numbers is minimal. 
\end{enumerate}
\subsection{k-Medians}
\begin{enumerate}
\item \centerfig{.5}{../figs/q3_3_1.pdf} The lowest error obtained over 50 runs is 1667.278.
\item Approaching the same methodology as in 3.2.4, I ran k-means 50 times, varying k between [1,10]. The following is the plot obtained showing the minimum error obtained in each k. \centerfig{.5}{../figs/q3_3_2.pdf} It is apparent, just like in 3.2.4, that the largest slope occured between k=1 and k=2, with value -3773.338. The same is true between k=2 and k=3, and k=3 and k=4. Beginning at k=4, however, the difference in error is minimal (${<}$10) when adding one more cluster. Thus, a k-value of 4 may be most appropriate for this dataset.
\item The k-medians algorithm can be found at \url{https://github.ubc.ca/cpsc340/jeanlam_nafis1_hw2/blob/master/code/kmedian.py}. The error obtained is 2082.490.
\item Under the elbow method with running k-median 50 times each over varying k-values of [1-10], the following graph was obtained: \centerfig{.5}{../figs/q3_3_4.pdf} Following the trend, the largest change in slope appears between k=1 and k=2, with a change of -4948.097. K=4 is the most appropriate choice as when k=5 was run, there was a larger error than k=4. 
\end{enumerate}

\section{Vector Quantization and Density-Based Clustering}
\subsection{Image Colour-Space Compression}
\begin{enumerate}
\item Link to code: 
\item Link to doge images:
\end{enumerate}
\subsection{Effect of Parameters on DBSCAN}
\begin{enumerate}
\item Keeping minPts = 3  The 4 true cluster -> radius =11
\item 3 clusters -> radius =18
\item 2 clusters -> radius =180
\item 1 clusters -> radius =800

\end{enumerate}
\subsection{K-Means vs. DBSCAN Clustering}
\begin{enumerate} 
\item I have chose r = 15 and kept min =15pts to get the following clusters:
\item{Cluster 1: antelope horse moose ox sheep giraffe buffalo zebra deer pig cow
Cluster 2: dalmatian persian+cat german+shepherd siamese+cat mole tiger leopard fox hamster squirrel rabbit wolf chihuahua rat weasel bobcat mouse collie
Cluster 3: hippopotamus elephant rhinoceros
Cluster 4: blue+whale humpback+whale seal walrus dolphin
Cluster 5: spider+monkey gorilla chimpanzee}
\end{enumerate}
\end{document}
API Training Shop Blog About
Â© 2017 GitHub, Inc. Help Support